{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtakLLx/OE8k87TUN4wANN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farheenfathimaa/NLP-with-Disaster-Tweets/blob/main/Natural_Language_Processing_with_Disaster_Tweets_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "metadata": {
        "id": "fFXce_zFwnoO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "218325f7-d324-421f-9a1b-cc4cc06df6b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the uploaded data into Google Drive\n",
        "#!unzip \"/content/drive/MyDrive/nlp-getting-started.zip\" -d \"/content/drive/MyDrive/nlp-tweets\""
      ],
      "metadata": {
        "id": "h5837so3gxKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing with Disaster Tweets\n",
        "\n",
        "This notebook looks into various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n",
        "\n",
        "We're going to take the following approach:\n",
        "\n",
        "1. Problem definition\n",
        "2. Data\n",
        "3. Evaluation\n",
        "4. Features\n",
        "5. Modelling\n",
        "6. Experimentation\n",
        "\n",
        "It is available on Kaggle. [Link](https://www.kaggle.com/competitions/nlp-getting-started/overview)\n",
        "\n",
        "## Tools"
      ],
      "metadata": {
        "id": "aqSgzH8Oo9Gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all the tools we need\n",
        "\n",
        "# Regular EDA (exploratory data analysis) and plotting libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Models from Scikit-Learn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Model Evaluations\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder #Encode categorical features as a one-hot numeric array.\n",
        "from sklearn.compose import ColumnTransformer # transform an entire column"
      ],
      "metadata": {
        "id": "fzJtqb_hvLJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# viewing the data\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/nlp-tweets/train.csv\")\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/nlp-tweets/test.csv\")\n",
        "train_data, test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfxIxocq9aeb",
        "outputId": "f4f875b8-bd00-469c-8302-ba7692e19cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(         id keyword location  \\\n",
              " 0         1     NaN      NaN   \n",
              " 1         4     NaN      NaN   \n",
              " 2         5     NaN      NaN   \n",
              " 3         6     NaN      NaN   \n",
              " 4         7     NaN      NaN   \n",
              " ...     ...     ...      ...   \n",
              " 7608  10869     NaN      NaN   \n",
              " 7609  10870     NaN      NaN   \n",
              " 7610  10871     NaN      NaN   \n",
              " 7611  10872     NaN      NaN   \n",
              " 7612  10873     NaN      NaN   \n",
              " \n",
              "                                                    text  target  \n",
              " 0     Our Deeds are the Reason of this #earthquake M...       1  \n",
              " 1                Forest fire near La Ronge Sask. Canada       1  \n",
              " 2     All residents asked to 'shelter in place' are ...       1  \n",
              " 3     13,000 people receive #wildfires evacuation or...       1  \n",
              " 4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
              " ...                                                 ...     ...  \n",
              " 7608  Two giant cranes holding a bridge collapse int...       1  \n",
              " 7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
              " 7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
              " 7611  Police investigating after an e-bike collided ...       1  \n",
              " 7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
              " \n",
              " [7613 rows x 5 columns],\n",
              "          id keyword location  \\\n",
              " 0         0     NaN      NaN   \n",
              " 1         2     NaN      NaN   \n",
              " 2         3     NaN      NaN   \n",
              " 3         9     NaN      NaN   \n",
              " 4        11     NaN      NaN   \n",
              " ...     ...     ...      ...   \n",
              " 3258  10861     NaN      NaN   \n",
              " 3259  10865     NaN      NaN   \n",
              " 3260  10868     NaN      NaN   \n",
              " 3261  10874     NaN      NaN   \n",
              " 3262  10875     NaN      NaN   \n",
              " \n",
              "                                                    text  \n",
              " 0                    Just happened a terrible car crash  \n",
              " 1     Heard about #earthquake is different cities, s...  \n",
              " 2     there is a forest fire at spot pond, geese are...  \n",
              " 3              Apocalypse lighting. #Spokane #wildfires  \n",
              " 4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
              " ...                                                 ...  \n",
              " 3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
              " 3259  Storm in RI worse than last hurricane. My city...  \n",
              " 3260  Green Line derailment in Chicago http://t.co/U...  \n",
              " 3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
              " 3262  #CityofCalgary has activated its Municipal Eme...  \n",
              " \n",
              " [3263 rows x 4 columns])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_data(df):\n",
        "  \"\"\"Transforms the data on which the model can be made\"\"\"\n",
        "  df[\"keyword\"].fillna(\"random\", inplace=True)\n",
        "  df[\"location\"].fillna(\"earth\", inplace=True)\n",
        "\n",
        "  # Convert the sparse matrix to a dense array\n",
        "  df = df.tonumpy()\n",
        "\n",
        "  # Convert sparse matrix to COO format\n",
        "  df_coo = df.tocoo()\n",
        "\n",
        "  # Iterate over rows and columns\n",
        "  for row, col, value in zip(df_coo.row, df_coo.col, df_coo.data):\n",
        "      # Do something with the row, column, and value\n",
        "      print(f\"Row: {row}, Column: {col}, Value: {value}\")\n",
        "\n",
        "  # This will turn all of the string value into category values\n",
        "  for label, content in df.items():\n",
        "      if pd.api.types.is_object_dtype(content):\n",
        "        df[label] = content.astype(\"category\").cat.as_ordered()\n",
        "\n",
        "  # turn the categories into numbers\n",
        "  categorical_features=[\"keyword\",\"location\",\"text\"]\n",
        "  one_hot=OneHotEncoder()\n",
        "  transformer=ColumnTransformer([(\"one_hot\",\n",
        "                                  one_hot,\n",
        "                                  categorical_features)],\n",
        "                                  remainder=\"passthrough\")\n",
        "  df=transformer.fit_transform(df) #fit_transform is a convenience method that combines the fitting and transformation steps into one call.\n",
        "  return df\n",
        "\n",
        "  def transform_data(df):\n",
        "    \"\"\"This function will turn all of the string value into category values\"\"\"\n",
        "    for col in df.columns:\n",
        "        if pd.api.types.is_object_dtype(df[col]):\n",
        "            df[col] = df[col].astype(\"category\").cat.as_ordered()\n",
        "    return df"
      ],
      "metadata": {
        "id": "06lT4GfB2-HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = transform_data(train_data)\n",
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "YYv0LP6uGO1N",
        "outputId": "b32ca491-4a2f-474c-be84-81cc32a886d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Index dimension must be 1 or 2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-0cfb735b42b1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-1f9cab76cb5c>\u001b[0m in \u001b[0;36mtransform_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;34m\"\"\"Transforms the data on which the model can be made\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"keyword\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"random\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"location\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"earth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Dispatch to specialized methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_validate_indices\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_bool_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"row\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_asindices\u001b[0;34m(self, idx, length)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Index dimension must be 1 or 2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Index dimension must be 1 or 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = transform_data(test_data)\n",
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "-npNdsh73yJX",
        "outputId": "24e269e0-c25a-4b7d-f9c0-8fda7c2930db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'train_data' referenced before assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f9486a383a2c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-0af38e408b08>\u001b[0m in \u001b[0;36mtransform_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# Convert the sparse matrix to a dense array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# This will turn all of the string value into category values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'train_data' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data is ready now we can make a model"
      ],
      "metadata": {
        "id": "Ar2335tb0sK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train,\n",
        "                                                    y_train,\n",
        "                                                    test_size=0.2)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "yXXcfa_44G4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Puts models in a dictionary\n",
        "models = {\"Logistic Regression\": LogisticRegression(),\n",
        "          \"KNN\": KNeighborsClassifier(),\n",
        "          \"Random Forest\": RandomForestClassifier()}\n",
        "\n",
        "# Create a function to fit and score models\n",
        "def fit_and_score(models, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Fits and evaluates given machine learnig models.\n",
        "    models : a dict of different Scikit-Learn machine learning models\n",
        "    X_train : training data (no labels)\n",
        "    X_test : testing data (no labels)\n",
        "    y_train : training labels\n",
        "    y_test : test labels\n",
        "    \"\"\"\n",
        "    # Set random seed\n",
        "    np.random.seed(42)\n",
        "    # Make a dictionary to keep the model scores\n",
        "    model_scores = {}\n",
        "    # Loop through models\n",
        "    for name, model in models.items():\n",
        "        # Fit the model to the data\n",
        "        model.fit(X_train,y_train)\n",
        "        # Evaluate the model and append it's score to the model_scores\n",
        "        model_scores[name]=model.score(X_test, y_test)\n",
        "    return model_scores"
      ],
      "metadata": {
        "id": "hYMtEJWA0c80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_scores = fit_and_score(models=models,\n",
        "                             X_train=X_train,\n",
        "                             X_test=X_test,\n",
        "                             y_train=y_train,\n",
        "                             y_test=y_test)\n",
        "model_scores"
      ],
      "metadata": {
        "id": "I0Dex94W43du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\n",
        "model_compare.T.plot.bar()"
      ],
      "metadata": {
        "id": "GNflt8Mo7fZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a hyperparameter grid for LogisticRegression\n",
        "log_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n",
        "                \"solver\": [\"liblinear\"]}\n",
        "\n",
        "# Create a hyperparameter grid for RandomForestClassifier\n",
        "rf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n",
        "           \"max_depth\": [None, 3, 5, 10],\n",
        "           \"min_samples_split\": np.arange(2, 20, 2),\n",
        "           \"min_samples_leaf\": np.arange(1,20,2)}"
      ],
      "metadata": {
        "id": "PgXLi7sJ7qdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tune LogisticRegression\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Setup random hyperparameter search for LogisticRegression\n",
        "rs_log_reg = RandomizedSearchCV(LogisticRegression(),\n",
        "                                param_distributions=log_reg_grid,\n",
        "                                cv=5,\n",
        "                                n_iter=20,\n",
        "                                verbose=True)\n",
        "\n",
        "# Fit random hyperparameter search model for LogisticRegression\n",
        "rs_log_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "aOeB73_H8Xkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs_log_reg.best_params_"
      ],
      "metadata": {
        "id": "NfYK45IA8bTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs_log_reg.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "1TKujkYs8ijR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# Setup random hyperparameter search for RandomForestClassifier()\n",
        "rs_rf = RandomizedSearchCV(RandomForestClassifier(),\n",
        "                           param_distributions=rf_grid,\n",
        "                           cv=5,\n",
        "                           n_iter=20,\n",
        "                           verbose=True)\n",
        "\n",
        "# Fit random hyperparameter search model for RandomForestClassifier()\n",
        "rs_rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "t5zBaTbS8l3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs_rf.best_params_"
      ],
      "metadata": {
        "id": "r9sOXVFw858X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs_rf.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "pjVKVfrk8-f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Different hyperparameters for our LogisticRegression model\n",
        "log_reg_grid = {\"C\": np.logspace(-4, 4, 30),\n",
        "                \"solver\": [\"liblinear\"]}\n",
        "\n",
        "#  Setup grid hyperparameter search for LogisticRegression\n",
        "gs_log_reg = GridSearchCV(LogisticRegression(),\n",
        "                          param_grid=log_reg_grid,\n",
        "                          cv=5,\n",
        "                          verbose=2)\n",
        "\n",
        "# Fit hyperparameter search for logisticRegression\n",
        "gs_log_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "NMtIzUGQ9FKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the best hyperparameters\n",
        "gs_log_reg.best_params_"
      ],
      "metadata": {
        "id": "jHLxCwD-BKwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the hyperparameter search for LogisticRegression\n",
        "gs_log_reg.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "aF_euVaKBO3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating our tuned machine learing classifier, beyond accuracy\n",
        "* ROC curve and AUC score\n",
        "* Confusion matrix\n",
        "* Classification report\n",
        "* Precision\n",
        "* Recall\n",
        "* F1-score"
      ],
      "metadata": {
        "id": "SH9At763BYYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with tuned model\n",
        "y_preds = gs_log_reg.predict(X_test)"
      ],
      "metadata": {
        "id": "FvrO2BOgBRXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds"
      ],
      "metadata": {
        "id": "LX-OuQrGBm-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "4BLbHlbCBo4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC curve and calculate AUC metric\n",
        "RocCurveDisplay.from_estimator(gs_log_reg, X_test, y_test);"
      ],
      "metadata": {
        "id": "FMWFUaWbBqz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(font_scale=1.5)\n",
        "\n",
        "def plot_conf_mat(y_test, y_preds):\n",
        "    \"\"\"\n",
        "    Plots a nice looking confusion matrix using Seaborn's heatmap()\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(3, 3))\n",
        "    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n",
        "                     annot=True,\n",
        "                     cbar=False)\n",
        "    plt.xlabel(\"Predicted label\")\n",
        "    plt.ylabel(\"True label\")\n",
        "\n",
        "plot_conf_mat(y_test, y_preds)"
      ],
      "metadata": {
        "id": "Hr6YyHY2B2bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_preds))"
      ],
      "metadata": {
        "id": "ZeArj0MOB8Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate evaluation metrics using cross-validation"
      ],
      "metadata": {
        "id": "zOAIC47iDC2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gs_log_reg.best_params_"
      ],
      "metadata": {
        "id": "yhHzAwGRCkBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new model with best params\n",
        "clf = LogisticRegression(C = 1.3738237958832638,\n",
        "                         solver = \"liblinear\")"
      ],
      "metadata": {
        "id": "558YYJ8KeHuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validated f1-score\n",
        "cv_f1 = cross_val_score(clf,\n",
        "                         transformed_X,\n",
        "                         y,\n",
        "                         cv=5,\n",
        "                         scoring=\"f1\")\n",
        "cv_f1 = np.mean(cv_f1)\n",
        "cv_f1"
      ],
      "metadata": {
        "id": "6aD2uy4AhNsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "qKC-11byh4i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.coef_"
      ],
      "metadata": {
        "id": "dnyUL8vF6Fx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Match coef's of features to columns\n",
        "feature_dict = dict(zip(train_data.columns, list(clf.coef_[0])))\n",
        "feature_dict"
      ],
      "metadata": {
        "id": "hNmMn4rr6KJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-xZRCxab6Wag"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}